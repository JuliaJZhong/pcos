# -*- coding: utf-8 -*-
"""preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16BJ9RUsr-SS5rT_4wHHteEtA6xp57ey3
"""

import anndata as ad
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import seaborn as sns
import scanpy as sc

data_path = '/home/jjzhong/projects/pcos/tensor_decomp/data'
raw_data_path = os.path.join(data_path, 'raw_data')

"""---
# from this point onward, following [this](https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html) tutorial!

## loading data
"""

samples = {} # dictionary with key = sample IDs, value = data path

for sample_id in os.listdir(raw_data_path):
  sample_path = os.path.join(raw_data_path, sample_id)
  subdir = os.listdir(sample_path)                              # i know there is only 1 subdir
  subsubdir = os.listdir(os.path.join(sample_path, subdir[0]))  # i know there is only 1 subsubdir

  path = os.path.join(sample_path, subdir[0], subsubdir[0])
  samples[sample_id] = path

adatas = {} # dictionary with key = sample IDs, value = AnnData object of scRNAseq data

for sample_id, path in samples.items():
  adata = sc.read_10x_mtx(path, var_names='gene_symbols', make_unique=True, cache=True)
  adatas[sample_id] = adata

adata = ad.concat(adatas, label="sample")
adata.obs_names_make_unique()
print('sample value counts:', adata.obs["sample"].value_counts())
print('-----')
print('initial anndata object:', adata)

# label the forskolin treated vs. control samples
treated = adata.obs["sample"].str.endswith("-F")

adata.obs["treatment"] = treated.map({True: "forskolin 20uM 24hr", False: "control"})

# label the PCOS vs. "healthy control" subjects
pcos_ids = ["Mc03", "Mc10", "Mc16", "Mc26", "Mc27"]
HC_ids = ["Mc02", "Mc06", "Mc31", "Mc40", "Mc50"] # not used but might as well have

adata.obs["donor_id"] = adata.obs["sample"].str.extract(r"(Mc\d+)", expand=False)

adata.obs["disease_status"] = adata.obs["donor_id"].map(
    lambda id: "PCOS" if id in pcos_ids else "HC"
)

print('-----')
print('treatment value counts:', adata.obs["treatment"].value_counts())
print('-----')
print('disease_status value counts:', adata.obs["disease_status"].value_counts())

# """## quality control - visualizing metrics and filtering"""

# mitochondrial genes, "MT-" for human, "Mt-" for mouse
adata.var["mt"] = adata.var_names.str.startswith("MT-")
# ribosomal genes
adata.var["ribo"] = adata.var_names.str.startswith(("RPS", "RPL"))
# hemoglobin genes
adata.var["hb"] = adata.var_names.str.contains("^HB[^(P)]")

# metrics saved in .obs and .var
sc.pp.calculate_qc_metrics(adata, qc_vars=["mt", "ribo", "hb"], inplace=True, log1p=True)

# n_genes_by_counts: number of genes with at least 1 count in a cell, calculated for all cells
# total_counts: sum of counts for each gene
# pct_counts_mt: percentage of total counts for a cell that are mitochondrial

sc.pl.violin(
    adata,
    ["n_genes_by_counts", "total_counts", "pct_counts_mt"],
    jitter=0.4,
    multi_panel=True,
    save='.png'
)

sc.pl.scatter(adata, "total_counts", "n_genes_by_counts", color="pct_counts_mt", save='.png')

# TODO: do i need to do this separately for each sample??? also potentially
# change to < 200 genes

# filter out cells with < 100 genes measured
sc.pp.filter_cells(adata, min_genes=100)

# filter out genes detected in less than 3 cells
sc.pp.filter_genes(adata, min_cells=3)

# filter out cells with > 5% of their genes being mitochondrial
adata = adata[adata.obs['pct_counts_mt'] < 5, :]

"""## detecting doublets"""

# assigns doublet_score and predicted_doublet to .obs based on nearest-neighbor classifer
# run scrublet on each batch separately, as doublets would only occur within each batch. assumption: each sample is a batch
sc.pp.scrublet(adata, knn_dist_metric='euclidean', batch_key='sample')
print('-----')
print('after labelling, filtering out cells with < 100 genes measured, filtering out genes detected in < 3 cells, filtering out cells with > 5% of their genes being mitochondrial:', adata)
adata = adata[adata.obs['predicted_doublet'] == False, :]
print('-----')
print('after filtering out predicted doublets:', adata)

sc.pl.scrublet_score_distribution(adata, save='.png')

"""## normalization"""

# store raw count matrix into a layer (wow this layer thing is super cool)
adata.layers["counts"] = adata.X.copy()

# also store current version of X and var as .raw.X and .raw.var
# right now, .X is raw counts but .var includes many variables added above
adata.raw = adata.copy()

# perform transformations on separate layer
# normalize to median total counts
# after normalization, each observation (cell) has a total count equal to
# the median of total counts for observations (cells) before normalization
sc.pp.normalize_total(adata)

# transforms data_value to log(1 + data_value)
sc.pp.log1p(adata)

"""## feature selection and dimensionality reduction via PCA"""

# batch key think about
# reproduces the implementations of Seurat, Cell Ranger, and Seurat v3
sc.pp.highly_variable_genes(adata, n_top_genes=2000)

# dispersion = degree of variation in gene expression across cells, relative to
# average expression for that gene
sc.pl.highly_variable_genes(adata)

# perform PCA on only the highly_variable_genes
# sc.tl.pca checks if sc.pp.highly_variable_genes was run previously
# defaults to 50 PCs
sc.tl.pca(adata, n_comps=20)

# # log scaling makes the y-axis harder to interpret, but it can make the elbow
# # easier to see
sc.pl.pca_variance_ratio(adata, n_pcs=20, log=True, save='.png')

sc.pl.pca(
    adata,
    color=["sample", "sample", "pct_counts_mt", "pct_counts_mt"],
    dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],  # principal components by index
    ncols=1,
    size=2,
    save='_1.png'
)

sc.pl.pca(
    adata,
    color=["treatment", "disease_status", "sample",],
    dimensions=[(0, 1), (0, 1), (0, 1)],  # principal components by index
    ncols=1,
    size=2,
    annotate_var_explained=True,
    save='_2.png'
)

# """## UMAP visualization"""

# # compute neighborhood graph of cells using PCA-transformed data
# # sc.pp.neighbors checks if sc.tl.pca was run on adata previously
sc.pp.neighbors(adata)

# # embed neighborhood graph in 2 dimensions for visualizing with UMAP
sc.tl.umap(adata)

# # by sample
# # "that is THE most batch effect-y UMAP i have ever seen" - CW
sc.pl.umap(
    adata,
    color="sample",
    # setting a smaller point size to get prevent overlap
    size=2,
    # legend_loc="on data"
    save='_1.png'
)

# # by treatment condition
sc.pl.umap(
    adata,
    color="treatment",
    # setting a smaller point size to get prevent overlap
    size=2,
    # legend_loc="on data"
    save='_2.png'
)

# # by disease status
sc.pl.umap(
    adata,
    color="disease_status",
    palette=["pink", "green"],
    # setting a smaller point size to get prevent overlap
    size=2,
    # legend_loc="on data"
    save='_3.png'
)

# """### the batch effects are insane here... export AnnData object in .h5ad to do some data integration with STACAS in R"""

# # TODO: consider moving this to top, because the .h5ad ends up being p large
# # does it need to have all the .obs and .var when i bring it into R?
# # i think R is ignoring / removing most of that stuff anyways
# # save to data folder with the day's date in filename
from datetime import date

today = date.today()
adata.write_h5ad(filename=data_path + '/adata_' + str(today) + '.h5ad' )